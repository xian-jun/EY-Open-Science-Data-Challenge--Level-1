{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import ipyleaflet\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Planetary Computer Tools\n",
    "import pystac\n",
    "import pystac_client\n",
    "import odc\n",
    "from pystac_client import Client\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "from odc.stac import stac_load\n",
    "import planetary_computer as pc\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import rich.table\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from scipy import stats\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download custom functions\n",
    "import sys\n",
    "sys.path.append('model_dev_functions')\n",
    "\n",
    "from data_prep import get_aggregation_from_window, batch_aggregate_pickle, read_multiple_pickles        # prepare dataset for model development\n",
    "from feature_engineering import add_NDVI, add_vhvv, get_high_corr_cols                                  # custom functions to help feature engineering\n",
    "from model_development import base_model_pred, model_scores, df_to_arr                                  # model development use\n",
    "from submission_format import prediction_to_submission_df                                               # change prediiction array of 0 and 1 to csv file suited for submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This notebook entails the cleaned up process of our journey to discover the final model that achieves 1.0 f1-score. To check out the final model, skip to section - `iteration 4`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Modelling exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the use of cross-validation, several models are explored to extract general principles for model development-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores(X, y, scoring):\n",
    "\n",
    "    # Random Forest classifier\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs = -1)\n",
    "    rf_scores = cross_val_score(rf_model, X, y, cv=5)\n",
    "\n",
    "    # Logistic Regression classifier\n",
    "    lr_model = LogisticRegression(random_state=42, n_jobs = -1)\n",
    "    lr_scores = cross_val_score(lr_model, X, y, cv=5, scoring = scoring)\n",
    "\n",
    "    # SVM classifier\n",
    "    svm_model = SVC(kernel='linear', random_state=42)\n",
    "    svm_scores = cross_val_score(svm_model, X, y, cv=5, scoring = scoring)\n",
    "\n",
    "    # XGBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs = -1)\n",
    "    xgb_scores = cross_val_score(xgb_model, X, y, cv=5, scoring = scoring)\n",
    "\n",
    "    # LightGBM classifier\n",
    "    lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42, n_jobs = -1)\n",
    "    lgb_scores = cross_val_score(lgb_model, X, y, cv=5, scoring = scoring)\n",
    "\n",
    "    return rf_scores, lr_scores, svm_scores, xgb_scores, lgb_scores\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several important general principles are extracted from the initial exploration on modelling.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principle 1: The use of window return better results than a single pixel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random samples 20210301 and 20210311 are chosen from the downloaded Sentinel-2 data and stacked together as features and tested against multiple machine learning models. In this case, windows with 5 pixels around each coordinate is averaged to get the aggregated window values representing the whole window. The performance of models are evaluated with by using the aggregated window values and the single pixel values as data input respectively. The models show improved accuracy with the use of aggregated mean value.\n",
    "\n",
    "Note: the window size of 5 is selected based on the analysis of distances between sampling points, of which we realise that in between points in the same cluster, most have distance of around 50m between points, corresponding to 5-pixels for the the highest spatial resolution of 10m for Sentinel-2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data extracted from Sentinel-2 bands on two dates 210301 and 210311\n",
    "s2_l2a_210301 = pd.read_pickle(\"../11-datasets/march-S2/LEVEL1_SENTINEL-2-L2A_2021-03-01.pkl\")\n",
    "s2_l2a_210311 = pd.read_pickle(\"../11-datasets/march-S2/LEVEL1_SENTINEL-2-L2A_2021-03-11.pkl\")\n",
    "\n",
    "# get columns that have just a single pixel values but not windows\n",
    "def get_all_one_point_bands(full_df, label_col,\n",
    "                            label_encode={'Rice': 1, 'Non Rice': 0}):\n",
    "    '''\n",
    "    all bands without windows retrieved by extracting the pixel value directly\n",
    "    '''\n",
    "    X = full_df[['B02', 'B03', 'B04', 'B08', 'B05', 'B06',\n",
    "                 'B07', 'B11', 'B12', 'B8A', 'SCL', 'B01', 'B09']]\n",
    "    y = full_df[label_col].map(label_encode)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "Xs_0301, y = get_all_one_point_bands(s2_l2a_210301, 'Class of Land')\n",
    "Xs_0311, y = get_all_one_point_bands(s2_l2a_210301, 'Class of Land')\n",
    "\n",
    "# concat data from 210301 and 210311\n",
    "Xs = pd.concat([Xs_0301, Xs_0311], axis=1).values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns with windows of 5*5 around the coordinate\n",
    "def get_aggregation_from_window(full_df, window_suffix, label_col, suffix_date, agg_method,\n",
    "                                label_encode={'Rice': 1, 'Non Rice': 0}):\n",
    "    '''\n",
    "    aggregate the w5 features to get a single value and return only these features\n",
    "\n",
    "    parameters:\n",
    "    full_df = the pickle file read to be processed\n",
    "    window_suffix = the window name suffix at the columns with windows, e.g. '_w5', '_w10'. Columns with window suffix '_w5' means it has a window of 5*5 pixels.\n",
    "    label_col = 'Class of Land'\n",
    "    suffix_date = date added to the processed columns as suffix to prevent column name clash after concatenating multiple dates\n",
    "    agg_method = aggregation method, e.g. lambda x: x.mean(), mode(), median(), etc.\n",
    "\n",
    "    return:\n",
    "    X, y \n",
    "    '''\n",
    "\n",
    "    windowed_columns = [\n",
    "        band for band in full_df.columns if window_suffix in band]\n",
    "\n",
    "    X = full_df[windowed_columns].applymap(agg_method)\n",
    "    X = X.add_suffix(suffix_date)\n",
    "\n",
    "    if label_col:\n",
    "        y = full_df[label_col].map(label_encode)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return X, y\n",
    "\n",
    "Xw_0301, y = get_aggregation_from_window(s2_l2a_210301, '_w5', 'Class of Land', '_0301', agg_method= lambda x: x.mean())\n",
    "Xw_0311, y = get_aggregation_from_window(s2_l2a_210311, '_w5', 'Class of Land', '_0311', agg_method= lambda x: x.mean())\n",
    "\n",
    "# concat data from 210301 and 210331\n",
    "Xw = pd.concat([Xw_0301, Xw_0311], axis=1).values\n",
    "y = y.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare F1-score of different models for single pixel values or aggregated pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_f1 (single pixel values)=  [0.9        0.85       0.96666667 0.95833333 0.825     ] , mean_f1  0.9\n",
      "lr_f1 (single pixel values) =  [0.98305085 0.89655172 0.91803279 0.79452055 0.88709677] , mean_f1=  0.8958505361239115\n",
      "svm_f1 (single pixel values) =  [0.95652174 0.91666667 0.91803279 0.80916031 0.90163934] , mean_f1 =  0.9004041684576309\n",
      "xgb_f1 (single pixel values) =  [0.928      0.88549618 0.95652174 0.94827586 0.859375  ] , mean_f1 =  0.9155337568811014\n",
      "lgb_f1 (single pixel values) =  [0.92561983 0.86363636 0.98305085 0.96551724 0.86153846] , mean_f1 =  0.9198725497445015\n"
     ]
    }
   ],
   "source": [
    "# f1 score (single pixel values)\n",
    "rf_base_stack, lr_base_stack, svm_base_stack, xgb_base_stack, lgb_base_stack = model_scores(Xs, y, 'f1')\n",
    "\n",
    "print('rf_f1 (single pixel values)= ', rf_base_stack, ', mean_f1 ', rf_base_stack.mean())\n",
    "print('lr_f1 (single pixel values) = ', lr_base_stack, ', mean_f1= ', lr_base_stack.mean())\n",
    "print('svm_f1 (single pixel values) = ', svm_base_stack, ', mean_f1 = ', svm_base_stack.mean())\n",
    "print('xgb_f1 (single pixel values) = ', xgb_base_stack, ', mean_f1 = ', xgb_base_stack.mean())\n",
    "print('lgb_f1 (single pixel values) = ', lgb_base_stack, ', mean_f1 = ', lgb_base_stack.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_f1 (aggregated pixel values) =  [1.         0.975      0.99166667 1.         0.64166667] , mean_f1 =  0.9216666666666666\n",
      "lr_f1 (aggregated pixel values) =  [0.98305085 0.92063492 0.99159664 0.93103448 0.89230769] , mean_f1 =  0.9437249163628646\n",
      "svm_f1 (aggregated pixel values) =  [0.96551724 0.89230769 0.98333333 0.92857143 0.77027027] , mean_f1 =  0.907999993172407\n",
      "xgb_f1 (aggregated pixel values) =  [1.         1.         1.         0.99159664 0.69822485] , mean_f1 =  0.9379642981452936\n",
      "lgb_f1 (aggregated pixel values) =  [1.         1.         1.         1.         0.90909091] , mean_f1 =  0.9818181818181818\n"
     ]
    }
   ],
   "source": [
    "# f1 score (aggregated pixel values)\n",
    "rf_mean_stack, lr_mean_stack, svm_mean_stack, xgb_mean_stack, lgb_mean_stack = model_scores(Xw, y, 'f1')\n",
    "\n",
    "print('rf_f1 (aggregated pixel values) = ', rf_mean_stack, ', mean_f1 = ', rf_mean_stack.mean())\n",
    "print('lr_f1 (aggregated pixel values) = ', lr_mean_stack, ', mean_f1 = ', lr_mean_stack.mean())\n",
    "print('svm_f1 (aggregated pixel values) = ', svm_mean_stack, ', mean_f1 = ', svm_mean_stack.mean())\n",
    "print('xgb_f1 (aggregated pixel values) = ', xgb_mean_stack, ', mean_f1 = ', xgb_mean_stack.mean())\n",
    "print('lgb_f1 (aggregated pixel values) = ', lgb_mean_stack, ', mean_f1 = ', lgb_mean_stack.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principle 2: Adding more dates as features will improve model performance (until they overfit) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same set of machine learning models, adding more dates as features improve the performance of features. This fits our common sense as crops go through multiple cycles throughout the year and more sampled dates result in more useful features informing the prediction model. \n",
    "\n",
    "In our case, we study this by setting a set of number of dates sampled to be used as our predictive features, i.e. 1 day, 2 days, 1 month, 3 months, 4 months. We do not use all available dates throughout the year to avoid overfitting as we only have 600 training samples. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data extracted from Sentinel-2 bands on two dates 210301 and 210311\n",
    "s2_l2a_210301 = pd.read_pickle(\"../11-datasets/march-S2/LEVEL1_SENTINEL-2-L2A_2021-03-01.pkl\")\n",
    "s2_l2a_210311 = pd.read_pickle(\"../11-datasets/march-S2/LEVEL1_SENTINEL-2-L2A_2021-03-11.pkl\")\n",
    "\n",
    "# 1 day (data from 210301)\n",
    "Xw_0301, y = get_aggregation_from_window(s2_l2a_210301, '_w5', 'Class of Land', '_0301', agg_method= lambda x: x.mean())\n",
    "\n",
    "\n",
    "# 2 days (concat data from 210301 and 210331)\n",
    "Xw_0311, y = get_aggregation_from_window(s2_l2a_210311, '_w5', 'Class of Land', '_0311', agg_method= lambda x: x.mean())\n",
    "Xw_2 = pd.concat([Xw_0301, Xw_0311], axis=1).values\n",
    "\n",
    "# 1 month (march)\n",
    "march_s2_paths, march_s2_dfs_list = read_multiple_pickles('../11-datasets/march-S2', ['latitude', 'longitude', 'geometry', 'grouping'])     # read_multiple_pickles is custom function to read all pickle files in folder and \n",
    "                                                                                                                                            # return a list of dfs needed as features along with ile_paths\n",
    "m_s2_df, m_s2_df_list  = batch_aggregate_pickle(march_s2_dfs_list, march_s2_paths, '_w5', 'Class of Land', agg_method=lambda x:x.mean())    # aggregate windows to get a single aggregated value for each window for a list of dfs\n",
    "m_s2_df = m_s2_df.drop('Class of Land', axis=1)\n",
    "\n",
    "# 3 months (february, august, december), justification elaborated in iteration 1\n",
    "fad_s2_paths, fad_s2_dfs_list = read_multiple_pickles('../11-datasets/feb_aug_dec-S2', ['latitude', 'longitude', 'geometry', 'grouping'])\n",
    "fad_s2_df, fad_s2_df_list  = batch_aggregate_pickle(fad_s2_dfs_list, fad_s2_paths, '_w5', 'Class of Land', agg_method=lambda x:x.mean())\n",
    "fad_s2_df = fad_s2_df.drop('Class of Land', axis=1)\n",
    "\n",
    "# 4 months (february, march, august, december)\n",
    "fmad_s2_df = pd.concat([m_s2_df, fad_s2_df], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing each datasets with our models (random forest, logistic regression, SVMClassifier, XGBoostClassifier, LightGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_f1 (1 days =  [0.95833333 0.875      0.975      0.94166667 0.8       ] , mean_f1 =  0.9099999999999999\n",
      "lr_f1 (1 day) =  [0.97435897 0.93333333 0.97435897 0.9047619  0.93548387] , mean_f1 =  0.9444594115561857\n",
      "svm_f1 (1 day) =  [0.94736842 0.896      0.98305085 0.875      0.85925926] , mean_f1 =  0.9121357055539037\n",
      "xgb_f1 (1 day) =  [0.95934959 0.90076336 0.97435897 0.94827586 0.70658683] , mean_f1 =  0.8978669230099612\n",
      "lgb_f1 (1 day) =  [1.         0.93442623 0.98305085 0.93913043 0.7195122 ] , mean_f1 =  0.9152239413740768\n"
     ]
    }
   ],
   "source": [
    "# f1 score (1 day)\n",
    "rf_mean_stack, lr_mean_stack, svm_mean_stack, xgb_mean_stack, lgb_mean_stack = model_scores(Xw_0301, y.values, 'f1')\n",
    "\n",
    "print('rf_f1 (1 days = ', rf_mean_stack, ', mean_f1 = ', rf_mean_stack.mean())\n",
    "print('lr_f1 (1 day) = ', lr_mean_stack, ', mean_f1 = ', lr_mean_stack.mean())\n",
    "print('svm_f1 (1 day) = ', svm_mean_stack, ', mean_f1 = ', svm_mean_stack.mean())\n",
    "print('xgb_f1 (1 day) = ', xgb_mean_stack, ', mean_f1 = ', xgb_mean_stack.mean())\n",
    "print('lgb_f1 (1 day) = ', lgb_mean_stack, ', mean_f1 = ', lgb_mean_stack.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_f1 (2 days) =  [1.         0.975      0.99166667 1.         0.64166667] , mean_f1 =  0.9216666666666666\n",
      "lr_f1 (2 days) =  [0.98305085 0.92063492 0.99159664 0.93103448 0.89230769] , mean_f1 =  0.9437249163628646\n",
      "svm_f1 (2 days) =  [0.96551724 0.89230769 0.98333333 0.92857143 0.77027027] , mean_f1 =  0.907999993172407\n",
      "xgb_f1 (2 days) =  [1.         1.         1.         0.99159664 0.69822485] , mean_f1 =  0.9379642981452936\n",
      "lgb_f1 (2 days) =  [1.         1.         1.         1.         0.90909091] , mean_f1 =  0.9818181818181818\n"
     ]
    }
   ],
   "source": [
    "# f1 score (2 days)\n",
    "rf_mean_stack, lr_mean_stack, svm_mean_stack, xgb_mean_stack, lgb_mean_stack = model_scores(Xw_2, y.values, 'f1')\n",
    "\n",
    "print('rf_f1 (2 days) = ', rf_mean_stack, ', mean_f1 = ', rf_mean_stack.mean())\n",
    "print('lr_f1 (2 days) = ', lr_mean_stack, ', mean_f1 = ', lr_mean_stack.mean())\n",
    "print('svm_f1 (2 days) = ', svm_mean_stack, ', mean_f1 = ', svm_mean_stack.mean())\n",
    "print('xgb_f1 (2 days) = ', xgb_mean_stack, ', mean_f1 = ', xgb_mean_stack.mean())\n",
    "print('lgb_f1 (2 days) = ', lgb_mean_stack, ', mean_f1 = ', lgb_mean_stack.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_f1 (2 days) =  [1.         1.         1.         1.         0.64166667] , mean_f1 =  0.9283333333333333\n",
      "lr_f1 (2 days) =  [0.96       0.88721805 0.98305085 0.92857143 0.82014388] , mean_f1 =  0.9157968412067848\n",
      "svm_f1 (2 days) =  [1.         0.921875   0.99159664 1.         0.77631579] , mean_f1 =  0.9379574856258293\n",
      "xgb_f1 (2 days) =  [1.         1.         1.         1.         0.70588235] , mean_f1 =  0.9411764705882353\n",
      "lgb_f1 (2 days) =  [1.         1.         0.98305085 1.         0.90909091] , mean_f1 =  0.9784283513097073\n"
     ]
    }
   ],
   "source": [
    "# f1 score (whole march)\n",
    "rf_mean_stack, lr_mean_stack, svm_mean_stack, xgb_mean_stack, lgb_mean_stack = model_scores(m_s2_df, y, 'f1')\n",
    "\n",
    "print('rf_f1 (2 days) = ', rf_mean_stack, ', mean_f1 = ', rf_mean_stack.mean())\n",
    "print('lr_f1 (2 days) = ', lr_mean_stack, ', mean_f1 = ', lr_mean_stack.mean())\n",
    "print('svm_f1 (2 days) = ', svm_mean_stack, ', mean_f1 = ', svm_mean_stack.mean())\n",
    "print('xgb_f1 (2 days) = ', xgb_mean_stack, ', mean_f1 = ', xgb_mean_stack.mean())\n",
    "print('lgb_f1 (2 days) = ', lgb_mean_stack, ', mean_f1 = ', lgb_mean_stack.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_f1 (2 days) =  [1.         1.         1.         1.         0.98333333] , mean_f1 =  0.9966666666666667\n",
      "lr_f1 (2 days) =  [1. 1. 1. 1. 1.] , mean_f1 =  1.0\n",
      "svm_f1 (2 days) =  [1.         1.         1.         1.         0.76433121] , mean_f1 =  0.9528662420382166\n",
      "xgb_f1 (2 days) =  [1.         0.99159664 1.         1.         1.        ] , mean_f1 =  0.9983193277310924\n",
      "lgb_f1 (2 days) =  [1.         0.99159664 1.         1.         1.        ] , mean_f1 =  0.9983193277310924\n"
     ]
    }
   ],
   "source": [
    "# f1 score (3 months: feb, aug, dec)\n",
    "rf_mean_stack, lr_mean_stack, svm_mean_stack, xgb_mean_stack, lgb_mean_stack = model_scores(fad_s2_df, y, 'f1')\n",
    "\n",
    "print('rf_f1 (2 days) = ', rf_mean_stack, ', mean_f1 = ', rf_mean_stack.mean())\n",
    "print('lr_f1 (2 days) = ', lr_mean_stack, ', mean_f1 = ', lr_mean_stack.mean())\n",
    "print('svm_f1 (2 days) = ', svm_mean_stack, ', mean_f1 = ', svm_mean_stack.mean())\n",
    "print('xgb_f1 (2 days) = ', xgb_mean_stack, ', mean_f1 = ', xgb_mean_stack.mean())\n",
    "print('lgb_f1 (2 days) = ', lgb_mean_stack, ', mean_f1 = ', lgb_mean_stack.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_f1 (2 days) =  [1.         1.         1.         1.         0.90833333] , mean_f1 =  0.9816666666666667\n",
      "lr_f1 (2 days) =  [1. 1. 1. 1. 1.] , mean_f1 =  1.0\n",
      "svm_f1 (2 days) =  [1. 1. 1. 1. 1.] , mean_f1 =  1.0\n",
      "xgb_f1 (2 days) =  [1.         0.99159664 1.         1.         1.        ] , mean_f1 =  0.9983193277310924\n",
      "lgb_f1 (2 days) =  [1.         0.99159664 1.         1.         1.        ] , mean_f1 =  0.9983193277310924\n"
     ]
    }
   ],
   "source": [
    "# f1 score (3 months: feb, march, aug, dec)\n",
    "rf_mean_stack, lr_mean_stack, svm_mean_stack, xgb_mean_stack, lgb_mean_stack = model_scores(fmad_s2_df, y, 'f1')\n",
    "\n",
    "print('rf_f1 (2 days) = ', rf_mean_stack, ', mean_f1 = ', rf_mean_stack.mean())\n",
    "print('lr_f1 (2 days) = ', lr_mean_stack, ', mean_f1 = ', lr_mean_stack.mean())\n",
    "print('svm_f1 (2 days) = ', svm_mean_stack, ', mean_f1 = ', svm_mean_stack.mean())\n",
    "print('xgb_f1 (2 days) = ', xgb_mean_stack, ', mean_f1 = ', xgb_mean_stack.mean())\n",
    "print('lgb_f1 (2 days) = ', lgb_mean_stack, ', mean_f1 = ', lgb_mean_stack.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the set of tests, we can notice a significant increase in the model accuracy by increasing the number of dates used as features. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principle 3: For the same input data, different model can show very different prediction. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be observed throughout the explorations above. It shows that model stacking might be able to help to make the model prediction to become more robust. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other explorations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many other exploration done via cross-validated experiments with training data, but on hindsight, they are omited as they do not provide much value to our actual improvements in test score. For instance: \n",
    "\n",
    "- model stacking\n",
    "- hyperparameter tuning\n",
    "- feature selection by removing highly correlated values\n",
    "- generate NDVI for each date\n",
    "- adding Sentinel-1 data ('vv', 'vh', 'vv/vh')\n",
    "\n",
    "\n",
    "This is due to the training samples being highly similar with one another and are not random enough, resuling in most of them having a very close to 1.0 f1-score when evaluated via cross-validation. This makes us being unable to see significant improvements or drop in f1-score with cross-validated evaluation. These techniques are further explored with the actual submissions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try with model with best performance- random forest (data feb, aug, dec). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../12-python_20230314\")\n",
    "\n",
    "from dataloader import l1_data_loader\n",
    "import geopandas as gpd\n",
    "import pystac\n",
    "import pystac_client\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point,box\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import contextily as ctx\n",
    "from pystac.extensions.eo import EOExtension as eo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1\n",
    "test accuracy = 0.89, <br>\n",
    "- Random Forest\n",
    "- all February, August, December data\n",
    "- window size of 5*5, aggregated by mean\n",
    "\n",
    " We think that the cropping season is where the rice crop differ the most from the other landcover the most. So we use these months - february august december based on the variance analysis on NDVI for rice crop. As a baseline model, this model only include Sentinel-2 data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image_analysis/NDVI variance analysis.png' alt=\"Alternative text\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of NDVI between rice and the mean of non-rice by dates is calculated. From this analysis, we found 3 peaks in the variance and choose to select the 3 months corresponding to them, i.e. February, August and December. (See LEVEL-1 data preparation for code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for february, march and december for Sentinel-2 (training data)\n",
    "\n",
    "# read multiple pickle files for band data corresponding to available dates in february, august and december\n",
    "fad_s2_paths, fad_s2_dfs_list = read_multiple_pickles('../11-datasets/feb_aug_dec-S2', ['latitude', 'longitude', 'geometry', 'grouping'])\n",
    "\n",
    "# aggregate all the features with windows 5*5 by mean for the list of dataframes read from the pickle files\n",
    "fad_s2_df, fad_s2_df_list  = batch_aggregate_pickle(fad_s2_dfs_list, fad_s2_paths, '_w5', 'Class of Land', agg_method=lambda x:x.mean())\n",
    "\n",
    "X_train1 = fad_s2_df.drop('Class of Land', axis=1)\n",
    "y_train1 = fad_s2_df['Class of Land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for february, march and december for Sentinel-2 (coordinates from submission template)\n",
    "sub2_fad_paths, sub2_fad_df_list = read_multiple_pickles('../11-datasets/SUBMISSION-template_data', \n",
    "                                                         ['id', 'latitude', 'longitude', 'geometry', 'target'], filter_condition='-03-')\n",
    "template_data_s2_df, template_s2_df_list = batch_aggregate_pickle(sub2_fad_df_list, sub2_fad_paths, '_w5', None)\n",
    "\n",
    "X_pred1 = template_data_s2_df.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model development and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model and get prediction result\n",
    "model1 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs = -1)\n",
    "model1.fit(fad_s2_df.drop('Class of Land', axis = 1), fad_s2_df['Class of Land'])\n",
    "\n",
    "pred1 = model1.predict(X_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(10.18019073690894, 105.32022315786804)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(10.561107033461816, 105.12772097986661)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10.623790611954897, 105.13771401411867)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(10.583364246115156, 105.23946127195805)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(10.20744446668854, 105.26844107128906)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>(10.308283266873062, 105.50872812216863)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>(10.582910017285496, 105.23991550078767)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>(10.581547330796518, 105.23991550078767)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>(10.629241357910818, 105.15315779432643)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>(10.574733898351617, 105.10410108072531)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           id    target\n",
       "0     (10.18019073690894, 105.32022315786804)      Rice\n",
       "1    (10.561107033461816, 105.12772097986661)      Rice\n",
       "2    (10.623790611954897, 105.13771401411867)      Rice\n",
       "3    (10.583364246115156, 105.23946127195805)  Non Rice\n",
       "4     (10.20744446668854, 105.26844107128906)      Rice\n",
       "..                                        ...       ...\n",
       "245  (10.308283266873062, 105.50872812216863)  Non Rice\n",
       "246  (10.582910017285496, 105.23991550078767)  Non Rice\n",
       "247  (10.581547330796518, 105.23991550078767)  Non Rice\n",
       "248  (10.629241357910818, 105.15315779432643)      Rice\n",
       "249  (10.574733898351617, 105.10410108072531)      Rice\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to dataframe for submission\n",
    "submission_df1 = prediction_to_submission_df('../submission/challenge_1_submission_template.csv', pred1)\n",
    "submission_df1\n",
    "\n",
    "# submission_df1.to_csv(\"L1_Submission_1.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2\n",
    "test accuracy = 0.86\n",
    "- all February, August, December data + NDVI\n",
    "- window size of 5*5, aggregated by mean\n",
    "- remove highly correlated features\n",
    "- stack models\n",
    "\n",
    " As we realize from our model development exploration that different models lead to different result, stacking models might lead to improved performance by making our result more robust. However, this reduces our performance instead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for february, march and december for Sentinel-2 (training data)\n",
    "\n",
    "# read multiple pickle files for band data corresponding to available dates in february, august and december\n",
    "fad_s2_paths, fad_s2_dfs_list = read_multiple_pickles('../11-datasets/feb_aug_dec-S2', ['latitude', 'longitude', 'geometry', 'grouping'])\n",
    "\n",
    "# aggregate all the features with windows 5*5 by mean for the list of dataframes read from the pickle files\n",
    "fad_s2_df, fad_s2_df_list  = batch_aggregate_pickle(fad_s2_dfs_list, fad_s2_paths, '_w5', 'Class of Land', agg_method=lambda x:x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for february, march and december for Sentinel-2 (coordinates from submission template)\n",
    "sub2_fad_paths, sub2_fad_df_list = read_multiple_pickles('../11-datasets/SUBMISSION-template_data', \n",
    "                                                         ['id', 'latitude', 'longitude', 'geometry', 'target'], filter_condition='-03-')\n",
    "template_data_s2_df, template_s2_df_list = batch_aggregate_pickle(sub2_fad_df_list, sub2_fad_paths, '_w5', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing \n",
    "to add NDVI and remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of high_corr_cols: 70\n"
     ]
    }
   ],
   "source": [
    "def train_2_processing(list_s2_to_add_NDVI, y_in, corr_thresh):\n",
    "    X2 = add_NDVI(list_s2_to_add_NDVI)\n",
    "    high_corr_cols = get_high_corr_cols(X2, corr_thresh)\n",
    "    X2 = X2.drop(high_corr_cols, axis=1)\n",
    "\n",
    "    y2 = y_in.copy()\n",
    "\n",
    "    return X2, y2, high_corr_cols\n",
    "\n",
    "X_train2, y_train2, corr_cols2 = train_2_processing(fad_s2_df_list, fad_s2_df['Class of Land'], corr_thresh = 0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AOT_w5_0209</th>\n",
       "      <th>B02_w5_0209</th>\n",
       "      <th>B08_w5_0209</th>\n",
       "      <th>WVP_w5_0209</th>\n",
       "      <th>visual_w5_0209</th>\n",
       "      <th>B11_w5_0209</th>\n",
       "      <th>SCL_w5_0209</th>\n",
       "      <th>B01_w5_0209</th>\n",
       "      <th>B09_w5_0209</th>\n",
       "      <th>NDVI_0</th>\n",
       "      <th>...</th>\n",
       "      <th>SCL_w5_1216</th>\n",
       "      <th>B09_w5_1216</th>\n",
       "      <th>NDVI_6</th>\n",
       "      <th>B02_w5_1226</th>\n",
       "      <th>B08_w5_1226</th>\n",
       "      <th>WVP_w5_1226</th>\n",
       "      <th>visual_w5_1226</th>\n",
       "      <th>SCL_w5_1226</th>\n",
       "      <th>B01_w5_1226</th>\n",
       "      <th>NDVI_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>204.0</td>\n",
       "      <td>586.320007</td>\n",
       "      <td>2676.879883</td>\n",
       "      <td>4240.600098</td>\n",
       "      <td>51.160000</td>\n",
       "      <td>1421.520020</td>\n",
       "      <td>4.00</td>\n",
       "      <td>787.280029</td>\n",
       "      <td>3860.560059</td>\n",
       "      <td>-0.686394</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>12270.919922</td>\n",
       "      <td>-0.069629</td>\n",
       "      <td>789.880005</td>\n",
       "      <td>1139.680054</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>89.760002</td>\n",
       "      <td>2.28</td>\n",
       "      <td>727.640015</td>\n",
       "      <td>-0.129313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204.0</td>\n",
       "      <td>379.640015</td>\n",
       "      <td>5017.359863</td>\n",
       "      <td>4734.040039</td>\n",
       "      <td>30.160000</td>\n",
       "      <td>1784.520020</td>\n",
       "      <td>4.00</td>\n",
       "      <td>373.440002</td>\n",
       "      <td>4630.279785</td>\n",
       "      <td>-0.890205</td>\n",
       "      <td>...</td>\n",
       "      <td>8.80</td>\n",
       "      <td>13114.280273</td>\n",
       "      <td>0.015639</td>\n",
       "      <td>895.440002</td>\n",
       "      <td>1312.160034</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>98.800003</td>\n",
       "      <td>7.00</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>-0.151119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204.0</td>\n",
       "      <td>858.719971</td>\n",
       "      <td>4147.040039</td>\n",
       "      <td>4255.000000</td>\n",
       "      <td>75.639999</td>\n",
       "      <td>2619.479980</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1945.239990</td>\n",
       "      <td>4604.359863</td>\n",
       "      <td>-0.697297</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>15611.559570</td>\n",
       "      <td>-0.073813</td>\n",
       "      <td>896.840027</td>\n",
       "      <td>1298.880005</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>83.680000</td>\n",
       "      <td>8.92</td>\n",
       "      <td>949.880005</td>\n",
       "      <td>-0.226979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204.0</td>\n",
       "      <td>1200.560059</td>\n",
       "      <td>688.080017</td>\n",
       "      <td>4377.000000</td>\n",
       "      <td>112.040001</td>\n",
       "      <td>689.159973</td>\n",
       "      <td>6.08</td>\n",
       "      <td>2009.199951</td>\n",
       "      <td>2318.120117</td>\n",
       "      <td>0.229456</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>16110.000000</td>\n",
       "      <td>-0.037297</td>\n",
       "      <td>1811.680054</td>\n",
       "      <td>1415.640015</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>222.440002</td>\n",
       "      <td>8.64</td>\n",
       "      <td>2333.879883</td>\n",
       "      <td>0.226469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204.0</td>\n",
       "      <td>286.239990</td>\n",
       "      <td>4420.959961</td>\n",
       "      <td>4496.520020</td>\n",
       "      <td>34.639999</td>\n",
       "      <td>1483.280029</td>\n",
       "      <td>4.00</td>\n",
       "      <td>208.199997</td>\n",
       "      <td>4497.319824</td>\n",
       "      <td>-0.859015</td>\n",
       "      <td>...</td>\n",
       "      <td>7.20</td>\n",
       "      <td>4733.200195</td>\n",
       "      <td>-0.306312</td>\n",
       "      <td>580.119995</td>\n",
       "      <td>4616.479980</td>\n",
       "      <td>4508.640137</td>\n",
       "      <td>45.119999</td>\n",
       "      <td>4.00</td>\n",
       "      <td>549.080017</td>\n",
       "      <td>-0.826212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>204.0</td>\n",
       "      <td>705.000000</td>\n",
       "      <td>258.440002</td>\n",
       "      <td>4377.000000</td>\n",
       "      <td>90.760002</td>\n",
       "      <td>147.440002</td>\n",
       "      <td>6.00</td>\n",
       "      <td>650.880005</td>\n",
       "      <td>117.680000</td>\n",
       "      <td>0.549442</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1142.959961</td>\n",
       "      <td>0.487675</td>\n",
       "      <td>5862.720215</td>\n",
       "      <td>5614.560059</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>9.00</td>\n",
       "      <td>4649.240234</td>\n",
       "      <td>-0.010744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>204.0</td>\n",
       "      <td>2149.760010</td>\n",
       "      <td>1895.760010</td>\n",
       "      <td>4377.000000</td>\n",
       "      <td>188.600006</td>\n",
       "      <td>1425.160034</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1920.959961</td>\n",
       "      <td>2152.199951</td>\n",
       "      <td>-0.011612</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>16110.000000</td>\n",
       "      <td>-0.039739</td>\n",
       "      <td>1522.479980</td>\n",
       "      <td>1028.160034</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>181.600006</td>\n",
       "      <td>8.76</td>\n",
       "      <td>2415.040039</td>\n",
       "      <td>0.269378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>204.0</td>\n",
       "      <td>2886.239990</td>\n",
       "      <td>2818.959961</td>\n",
       "      <td>4377.000000</td>\n",
       "      <td>229.559998</td>\n",
       "      <td>1935.599976</td>\n",
       "      <td>7.88</td>\n",
       "      <td>1835.000000</td>\n",
       "      <td>2008.359985</td>\n",
       "      <td>-0.064964</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>16110.000000</td>\n",
       "      <td>-0.029654</td>\n",
       "      <td>1568.079956</td>\n",
       "      <td>1149.000000</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>160.919998</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1867.319946</td>\n",
       "      <td>0.158007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>204.0</td>\n",
       "      <td>683.359985</td>\n",
       "      <td>2796.239990</td>\n",
       "      <td>4598.720215</td>\n",
       "      <td>61.919998</td>\n",
       "      <td>1583.920044</td>\n",
       "      <td>4.00</td>\n",
       "      <td>696.799988</td>\n",
       "      <td>2879.560059</td>\n",
       "      <td>-0.644441</td>\n",
       "      <td>...</td>\n",
       "      <td>8.44</td>\n",
       "      <td>10499.919922</td>\n",
       "      <td>-0.003553</td>\n",
       "      <td>990.520020</td>\n",
       "      <td>1584.319946</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>95.360001</td>\n",
       "      <td>10.00</td>\n",
       "      <td>978.039978</td>\n",
       "      <td>-0.258276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>204.0</td>\n",
       "      <td>401.920013</td>\n",
       "      <td>4230.080078</td>\n",
       "      <td>4570.680176</td>\n",
       "      <td>34.119999</td>\n",
       "      <td>2088.760010</td>\n",
       "      <td>4.00</td>\n",
       "      <td>350.679993</td>\n",
       "      <td>4114.799805</td>\n",
       "      <td>-0.854843</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>16109.400391</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>578.919983</td>\n",
       "      <td>810.039978</td>\n",
       "      <td>4347.000000</td>\n",
       "      <td>68.680000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>567.359985</td>\n",
       "      <td>-0.093407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AOT_w5_0209  B02_w5_0209  B08_w5_0209  WVP_w5_0209  visual_w5_0209  \\\n",
       "0          204.0   586.320007  2676.879883  4240.600098       51.160000   \n",
       "1          204.0   379.640015  5017.359863  4734.040039       30.160000   \n",
       "2          204.0   858.719971  4147.040039  4255.000000       75.639999   \n",
       "3          204.0  1200.560059   688.080017  4377.000000      112.040001   \n",
       "4          204.0   286.239990  4420.959961  4496.520020       34.639999   \n",
       "..           ...          ...          ...          ...             ...   \n",
       "245        204.0   705.000000   258.440002  4377.000000       90.760002   \n",
       "246        204.0  2149.760010  1895.760010  4377.000000      188.600006   \n",
       "247        204.0  2886.239990  2818.959961  4377.000000      229.559998   \n",
       "248        204.0   683.359985  2796.239990  4598.720215       61.919998   \n",
       "249        204.0   401.920013  4230.080078  4570.680176       34.119999   \n",
       "\n",
       "     B11_w5_0209  SCL_w5_0209  B01_w5_0209  B09_w5_0209    NDVI_0  ...  \\\n",
       "0    1421.520020         4.00   787.280029  3860.560059 -0.686394  ...   \n",
       "1    1784.520020         4.00   373.440002  4630.279785 -0.890205  ...   \n",
       "2    2619.479980         4.40  1945.239990  4604.359863 -0.697297  ...   \n",
       "3     689.159973         6.08  2009.199951  2318.120117  0.229456  ...   \n",
       "4    1483.280029         4.00   208.199997  4497.319824 -0.859015  ...   \n",
       "..           ...          ...          ...          ...       ...  ...   \n",
       "245   147.440002         6.00   650.880005   117.680000  0.549442  ...   \n",
       "246  1425.160034         7.36  1920.959961  2152.199951 -0.011612  ...   \n",
       "247  1935.599976         7.88  1835.000000  2008.359985 -0.064964  ...   \n",
       "248  1583.920044         4.00   696.799988  2879.560059 -0.644441  ...   \n",
       "249  2088.760010         4.00   350.679993  4114.799805 -0.854843  ...   \n",
       "\n",
       "     SCL_w5_1216   B09_w5_1216    NDVI_6  B02_w5_1226  B08_w5_1226  \\\n",
       "0           9.00  12270.919922 -0.069629   789.880005  1139.680054   \n",
       "1           8.80  13114.280273  0.015639   895.440002  1312.160034   \n",
       "2           9.00  15611.559570 -0.073813   896.840027  1298.880005   \n",
       "3           9.00  16110.000000 -0.037297  1811.680054  1415.640015   \n",
       "4           7.20   4733.200195 -0.306312   580.119995  4616.479980   \n",
       "..           ...           ...       ...          ...          ...   \n",
       "245        10.00   1142.959961  0.487675  5862.720215  5614.560059   \n",
       "246         9.00  16110.000000 -0.039739  1522.479980  1028.160034   \n",
       "247         9.00  16110.000000 -0.029654  1568.079956  1149.000000   \n",
       "248         8.44  10499.919922 -0.003553   990.520020  1584.319946   \n",
       "249         9.00  16109.400391  0.009493   578.919983   810.039978   \n",
       "\n",
       "     WVP_w5_1226  visual_w5_1226  SCL_w5_1226  B01_w5_1226    NDVI_7  \n",
       "0    4347.000000       89.760002         2.28   727.640015 -0.129313  \n",
       "1    4347.000000       98.800003         7.00   871.000000 -0.151119  \n",
       "2    4347.000000       83.680000         8.92   949.880005 -0.226979  \n",
       "3    4347.000000      222.440002         8.64  2333.879883  0.226469  \n",
       "4    4508.640137       45.119999         4.00   549.080017 -0.826212  \n",
       "..           ...             ...          ...          ...       ...  \n",
       "245  4347.000000      255.000000         9.00  4649.240234 -0.010744  \n",
       "246  4347.000000      181.600006         8.76  2415.040039  0.269378  \n",
       "247  4347.000000      160.919998         9.12  1867.319946  0.158007  \n",
       "248  4347.000000       95.360001        10.00   978.039978 -0.258276  \n",
       "249  4347.000000       68.680000         3.00   567.359985 -0.093407  \n",
       "\n",
       "[250 rows x 66 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pred_2_processing(list_s2_to_add_NDVI, corr_cols):\n",
    "    X2 = add_NDVI(list_s2_to_add_NDVI)\n",
    "    X2 = X2.drop(corr_cols, axis=1)\n",
    "\n",
    "    return X2\n",
    "\n",
    "X_pred2 = pred_2_processing(template_s2_df_list, corr_cols2)\n",
    "X_pred2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model development and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train base models\n",
    "models = [\n",
    "    RandomForestClassifier(random_state = 42, n_estimators=100),\n",
    "    LogisticRegression(random_state = 42 ),\n",
    "    #xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs = -1),\n",
    "    lgb.LGBMClassifier(random_state = 42, n_estimators=100)\n",
    "]\n",
    "\n",
    "\n",
    "def base_model_pred_for_submission(X_train, y_train, X_pred, models):\n",
    "    '''\n",
    "    Use each model in the list of models to predict label based on X_pred,\n",
    "    with each predicted array combined to form a resultant array with the shape of (250, number_of_models)\n",
    "    '''\n",
    "    # Generate predictions from base models\n",
    "    X = []  # This will store the prediction outputs of each model\n",
    "    y = []  # This will store the true labels\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_pred)\n",
    "        X.append(y_pred)\n",
    "\n",
    "    X_2 = np.array(X).T  # Convert to a 2D array, where each row represents a sample and each column represents a model's prediction\n",
    "\n",
    "    return X_2 \n",
    "\n",
    "X_pred2_2 = base_model_pred_for_submission(X_train2, y_train2, X_pred2, models)\n",
    "X_pred2_2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "def base_model_pred(X_train, y_train, models):\n",
    "    '''\n",
    "    Use each model in the list of models to predict label based on X_train,\n",
    "    with each predicted array combined to form a resultant array with the shape of (600, number_of_models).\n",
    "\n",
    "    Used to train the 2nd layer of model.\n",
    "    '''\n",
    "    # Generate predictions from base models\n",
    "    X = []  # This will store the prediction outputs of each model\n",
    "    y = []  # This will store the true labels\n",
    "    for model in models:\n",
    "        y_pred = cross_val_predict(model, X_train, y_train, cv=5)\n",
    "        X.append(y_pred)\n",
    "\n",
    "    # Convert to a 2D array, where each row represents a sample and each column represents a model's prediction\n",
    "    X = np.array(X).T\n",
    "    y = y_train  # Flatten the true labels into a 1D array\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# train 2nd layer model\n",
    "X_train2_2, y_train2_2 = base_model_pred(X_train2, y_train2, models)\n",
    "models[0].fit(X_train2_2, y_train2_2)\n",
    "meta_rf = models[0].predict(X_pred2_2) \n",
    "\n",
    "# final prediction\n",
    "pred2 = meta_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(10.18019073690894, 105.32022315786804)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(10.561107033461816, 105.12772097986661)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10.623790611954897, 105.13771401411867)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(10.583364246115156, 105.23946127195805)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(10.20744446668854, 105.26844107128906)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>(10.308283266873062, 105.50872812216863)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>(10.582910017285496, 105.23991550078767)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>(10.581547330796518, 105.23991550078767)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>(10.629241357910818, 105.15315779432643)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>(10.574733898351617, 105.10410108072531)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           id    target\n",
       "0     (10.18019073690894, 105.32022315786804)      Rice\n",
       "1    (10.561107033461816, 105.12772097986661)      Rice\n",
       "2    (10.623790611954897, 105.13771401411867)  Non Rice\n",
       "3    (10.583364246115156, 105.23946127195805)  Non Rice\n",
       "4     (10.20744446668854, 105.26844107128906)      Rice\n",
       "..                                        ...       ...\n",
       "245  (10.308283266873062, 105.50872812216863)  Non Rice\n",
       "246  (10.582910017285496, 105.23991550078767)  Non Rice\n",
       "247  (10.581547330796518, 105.23991550078767)  Non Rice\n",
       "248  (10.629241357910818, 105.15315779432643)      Rice\n",
       "249  (10.574733898351617, 105.10410108072531)      Rice\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load submission\n",
    "submission_df2 = prediction_to_submission_df('../submission/challenge_1_submission_template.csv', pred2)\n",
    "submission_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_df2.to_csv(\"L1_Submission_2.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 3\n",
    "test score = 0.94 / 0.95\n",
    "\n",
    "we tried adding Sentinel-1 data to the model stacking method used in iteration 2, which improves our model performance significantly. F1-score = 0.94 /  0.95 based on the normalization method we use (MinMax and RobustScaler performs better). To help readability, we will only illustrate experimentation with MinMaxScaler. \n",
    "\n",
    "- Sentinel-2: Feb, Aug, Dec raw data + NDVI \n",
    "- Sentinel-1: Sentinel-1 raw data data + VH / VV\n",
    "- drop highly correlated features\n",
    "- normalization (StandardScaler, MinMaxScaler, RobustScaler)\n",
    "- stack models\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for february, august and december for Sentinel-2 (training data)\n",
    "fad_s2_paths, fad_s2_dfs_list = read_multiple_pickles('../11-datasets/feb_aug_dec-S2', ['latitude', 'longitude', 'geometry', 'grouping'])     # read multiple pickle files for band data corresponding to available dates in february, august and december\n",
    "fad_s2_df, fad_s2_df_list  = batch_aggregate_pickle(fad_s2_dfs_list, fad_s2_paths, '_w5', 'Class of Land', agg_method=lambda x:x.mean())      # aggregate all the features with windows 5*5 by mean for the list of dataframes read from the pickle files\n",
    "\n",
    "# get data for february, august and december for Sentinel-1 (training data)\n",
    "fad_s1_paths, fad_s1_dfs_list = read_multiple_pickles('../11-datasets/sentinel1a_data', \n",
    "                                                        ['latitude', 'longitude', 'geometry', 'grouping'], filter_condition='-03-')     # read multiple pickle files for band data corresponding to available dates in february, august and december\n",
    "fad_s1_df, fad_s1_df_list  = batch_aggregate_pickle(fad_s1_dfs_list, fad_s1_paths, '_w5', 'Class of Land', agg_method=lambda x:x.mean())      # aggregate all the features with windows 5*5 by mean for the list of dataframes read from the pickle files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "template data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for february, march and december for Sentinel-2 (coordinates from submission template)\n",
    "sub2_fmad_paths, sub2_fmad_df_list = read_multiple_pickles('../11-datasets/SUBMISSION-template_data', \n",
    "                                                         ['id', 'latitude', 'longitude', 'geometry', 'target'], filter_condition='-03-')\n",
    "template_data_s2_df, template_s2_df_list = batch_aggregate_pickle(sub2_fmad_df_list, sub2_fmad_paths, '_w5', None)\n",
    "\n",
    "# get data for february, march and december for Sentinel-1\n",
    "sub1_fad_paths, sub1_fad_df_list = read_multiple_pickles('../11-datasets/SUBMISSION-template_data/sentinel1a_template_data', \n",
    "                                                         ['id', 'latitude', 'longitude', 'geometry', 'target'], filter_condition='-03-')\n",
    "template_data_s1_df, template_s1_df_list = batch_aggregate_pickle(sub1_fad_df_list, sub1_fad_paths, '_w5', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of high_corr_cols: 70\n"
     ]
    }
   ],
   "source": [
    "def train_3_processing(list_s2_to_add_NDVI, s1_X_in, y_in, corr_thresh):\n",
    "    X3 = pd.concat([add_NDVI(list_s2_to_add_NDVI), s1_X_in], axis=1)\n",
    "    high_corr_cols = get_high_corr_cols(X3, corr_thresh)\n",
    "    X3 = X3.drop(high_corr_cols, axis=1)\n",
    "\n",
    "    y3 = y_in.copy()\n",
    "\n",
    "    return X3, y3, high_corr_cols\n",
    "\n",
    "X_train3_prescaled, y_train3, corr_cols3 =  train_3_processing(fad_s2_df_list, add_vhvv(fad_s1_df_list), fad_s1_df['Class of Land'], corr_thresh = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_3_processing(list_s2_to_add_NDVI, sub1_X_in, corr_cols):\n",
    "    X3 = pd.concat([add_NDVI(list_s2_to_add_NDVI), sub1_X_in], axis=1)\n",
    "    X3 = X3.drop(corr_cols, axis=1)\n",
    "\n",
    "    return X3\n",
    "\n",
    "X_pred3_prescaled = pred_3_processing(template_s2_df_list, add_vhvv(template_s1_df_list), corr_cols3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_pred3_prescaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "scale.fit(X_train3_prescaled)\n",
    "\n",
    "X_train3= scale.transform(X_train3_prescaled)\n",
    "X_pred3 = scale.transform(X_pred3_prescaled)\n",
    "y_train3 = y_train3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model development and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train base models\n",
    "models = [\n",
    "    RandomForestClassifier(random_state = 42, n_estimators=100),\n",
    "    LogisticRegression(random_state = 42 ),\n",
    "    #xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs = -1),\n",
    "    lgb.LGBMClassifier(random_state = 42, n_estimators=100)\n",
    "]\n",
    "\n",
    "\n",
    "def base_model_pred_for_submission(X_train, y_train, X_pred, models):\n",
    "    # Generate predictions from base models\n",
    "    X = []  # This will store the prediction outputs of each model\n",
    "    y = []  # This will store the true labels\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_pred)\n",
    "        X.append(y_pred)\n",
    "\n",
    "    X = np.array(X).T  # Convert to a 2D array, where each row represents a sample and each column represents a model's prediction\n",
    "    # y = y_train  # Flatten the true labels into a 1D array\n",
    "\n",
    "    return X #, y\n",
    "\n",
    "X_pred3_2 = base_model_pred_for_submission(X_train3, y_train3, X_pred3, models)\n",
    "X_pred3_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3_2, y_train3_2 = base_model_pred(X_train3, y_train3, models)\n",
    "\n",
    "models[0].fit(X_train3_2, y_train3_2)\n",
    "meta_rf = models[0].predict(X_pred3_2) \n",
    "\n",
    "# models[1].fit(X_train2_2, y_train2_2)\n",
    "# meta_lr = models[1].predict(X_pred2_2)\n",
    "\n",
    "# pred2 = (meta_rf + meta_lr) / 2\n",
    "pred3 = meta_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load submission\n",
    "submission_df3 = prediction_to_submission_df('../submission/challenge_1_submission_template.csv', pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_df3.to_csv(\"L1_Submission_3.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 4\n",
    "test_score = 1.0\n",
    "\n",
    "As we observe that the inclusion of Sentinel-1 data in `iteration 3` significantly improves our prediction accuracy, we assume that cloud is the most important factor in this classification task. We conduct cloud analysis and find out the dates with the least amount of cloud throughout the year. Based on the dates, we extract their corresponding Sentinel-1 and Sentinel-2 data. increased window size to 9*9 to reduce the impact of possible cloud covering the window. \n",
    "- cloud15 data\n",
    "- drop highly correlated features\n",
    "- normalization (Robust)\n",
    "- stack models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud cover analysis to extract scenes with low cloud coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image_analysis/cloud cover analysis.png' alt=\"Alternative text\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that the Sentinel data does not cover all groups of training data in a single scene, we break down the analysis into East scene and West scene. East scene consists of all groups from training data other than the group furthest to the West. \n",
    "\n",
    "With this analysis, we can select satellite data from dates with less than 15% cloud cover over our training samples. As East side scenes contains most of the data groups (i.e. 6 of 7 in the east), therefore east side will be use as the primary critiria. \n",
    "\n",
    "As we know that the training data is in the same region as the submission template data, we think that it is reasonable to assume that the dates with less cloud cover for training data means the same for the submission template data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AOT_w9_0115</th>\n",
       "      <th>B02_w9_0115</th>\n",
       "      <th>B03_w9_0115</th>\n",
       "      <th>B04_w9_0115</th>\n",
       "      <th>B08_w9_0115</th>\n",
       "      <th>WVP_w9_0115</th>\n",
       "      <th>visual_w9_0115</th>\n",
       "      <th>B05_w9_0115</th>\n",
       "      <th>B06_w9_0115</th>\n",
       "      <th>B07_w9_0115</th>\n",
       "      <th>...</th>\n",
       "      <th>vh_w9_0520</th>\n",
       "      <th>vv_w9_0520</th>\n",
       "      <th>vh_w9_0601</th>\n",
       "      <th>vv_w9_0601</th>\n",
       "      <th>vh_w9_0818</th>\n",
       "      <th>vv_w9_0818</th>\n",
       "      <th>vh_w9_1205</th>\n",
       "      <th>vv_w9_1205</th>\n",
       "      <th>vh_w9_1229</th>\n",
       "      <th>vv_w9_1229</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.0</td>\n",
       "      <td>620.629639</td>\n",
       "      <td>967.333313</td>\n",
       "      <td>463.370361</td>\n",
       "      <td>3964.024658</td>\n",
       "      <td>3326.357910</td>\n",
       "      <td>47.592594</td>\n",
       "      <td>1186.308594</td>\n",
       "      <td>3340.926025</td>\n",
       "      <td>4031.987549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.157805</td>\n",
       "      <td>0.031072</td>\n",
       "      <td>0.078085</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.038904</td>\n",
       "      <td>0.036330</td>\n",
       "      <td>0.177955</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>0.037547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138.0</td>\n",
       "      <td>676.679016</td>\n",
       "      <td>904.086426</td>\n",
       "      <td>496.061737</td>\n",
       "      <td>3089.407471</td>\n",
       "      <td>3345.357910</td>\n",
       "      <td>50.901234</td>\n",
       "      <td>1211.271606</td>\n",
       "      <td>2910.061768</td>\n",
       "      <td>3468.333252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028177</td>\n",
       "      <td>0.349164</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.092954</td>\n",
       "      <td>0.008786</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.055847</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>0.055702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138.0</td>\n",
       "      <td>594.518494</td>\n",
       "      <td>899.370361</td>\n",
       "      <td>453.098755</td>\n",
       "      <td>3599.037109</td>\n",
       "      <td>3260.691406</td>\n",
       "      <td>46.592594</td>\n",
       "      <td>1132.012329</td>\n",
       "      <td>2945.061768</td>\n",
       "      <td>3540.209961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036251</td>\n",
       "      <td>0.199289</td>\n",
       "      <td>0.031167</td>\n",
       "      <td>0.148355</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.153203</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>0.022632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138.0</td>\n",
       "      <td>612.617310</td>\n",
       "      <td>969.024719</td>\n",
       "      <td>457.345673</td>\n",
       "      <td>4026.049316</td>\n",
       "      <td>3344.580322</td>\n",
       "      <td>46.987656</td>\n",
       "      <td>1221.802490</td>\n",
       "      <td>3488.481445</td>\n",
       "      <td>4222.728516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029057</td>\n",
       "      <td>0.136654</td>\n",
       "      <td>0.024184</td>\n",
       "      <td>0.078517</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.050735</td>\n",
       "      <td>0.049119</td>\n",
       "      <td>0.186494</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.067469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138.0</td>\n",
       "      <td>591.839478</td>\n",
       "      <td>699.641968</td>\n",
       "      <td>391.703705</td>\n",
       "      <td>2411.037109</td>\n",
       "      <td>3597.086426</td>\n",
       "      <td>40.271606</td>\n",
       "      <td>754.666687</td>\n",
       "      <td>1883.036987</td>\n",
       "      <td>2293.296387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032237</td>\n",
       "      <td>0.182050</td>\n",
       "      <td>0.029203</td>\n",
       "      <td>0.071724</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.096539</td>\n",
       "      <td>0.008988</td>\n",
       "      <td>0.039027</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.032278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>138.0</td>\n",
       "      <td>3547.037109</td>\n",
       "      <td>3536.123535</td>\n",
       "      <td>3399.456787</td>\n",
       "      <td>5190.913574</td>\n",
       "      <td>3244.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>4135.827148</td>\n",
       "      <td>4902.740723</td>\n",
       "      <td>5308.320801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077351</td>\n",
       "      <td>0.296637</td>\n",
       "      <td>0.089087</td>\n",
       "      <td>0.326931</td>\n",
       "      <td>0.072536</td>\n",
       "      <td>0.369975</td>\n",
       "      <td>0.058265</td>\n",
       "      <td>0.278032</td>\n",
       "      <td>0.052680</td>\n",
       "      <td>0.255063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>138.0</td>\n",
       "      <td>3677.234619</td>\n",
       "      <td>3715.728516</td>\n",
       "      <td>3639.283936</td>\n",
       "      <td>5169.777832</td>\n",
       "      <td>3244.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>4065.308594</td>\n",
       "      <td>4804.962891</td>\n",
       "      <td>5194.530762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072449</td>\n",
       "      <td>0.313707</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.358288</td>\n",
       "      <td>0.075839</td>\n",
       "      <td>0.311691</td>\n",
       "      <td>0.066887</td>\n",
       "      <td>0.242163</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.226601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>138.0</td>\n",
       "      <td>3955.530762</td>\n",
       "      <td>3922.024658</td>\n",
       "      <td>3810.567871</td>\n",
       "      <td>5253.185059</td>\n",
       "      <td>3244.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>4083.481445</td>\n",
       "      <td>4751.197754</td>\n",
       "      <td>5104.567871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071250</td>\n",
       "      <td>0.307097</td>\n",
       "      <td>0.079684</td>\n",
       "      <td>0.337885</td>\n",
       "      <td>0.077614</td>\n",
       "      <td>0.263658</td>\n",
       "      <td>0.065535</td>\n",
       "      <td>0.248294</td>\n",
       "      <td>0.080965</td>\n",
       "      <td>0.230987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>138.0</td>\n",
       "      <td>3752.493896</td>\n",
       "      <td>3682.567871</td>\n",
       "      <td>3530.469238</td>\n",
       "      <td>4916.493652</td>\n",
       "      <td>3244.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>4017.938232</td>\n",
       "      <td>4671.617188</td>\n",
       "      <td>5010.147949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086430</td>\n",
       "      <td>0.282350</td>\n",
       "      <td>0.074414</td>\n",
       "      <td>0.339348</td>\n",
       "      <td>0.073026</td>\n",
       "      <td>0.258110</td>\n",
       "      <td>0.065576</td>\n",
       "      <td>0.230867</td>\n",
       "      <td>0.078133</td>\n",
       "      <td>0.258629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>138.0</td>\n",
       "      <td>3206.246826</td>\n",
       "      <td>3110.123535</td>\n",
       "      <td>2891.407471</td>\n",
       "      <td>4446.296387</td>\n",
       "      <td>3244.000000</td>\n",
       "      <td>239.876541</td>\n",
       "      <td>3738.222168</td>\n",
       "      <td>4432.777832</td>\n",
       "      <td>4786.654297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074432</td>\n",
       "      <td>0.297338</td>\n",
       "      <td>0.074617</td>\n",
       "      <td>0.320459</td>\n",
       "      <td>0.077615</td>\n",
       "      <td>0.333372</td>\n",
       "      <td>0.066109</td>\n",
       "      <td>0.284618</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.308956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AOT_w9_0115  B02_w9_0115  B03_w9_0115  B04_w9_0115  B08_w9_0115  \\\n",
       "0          138.0   620.629639   967.333313   463.370361  3964.024658   \n",
       "1          138.0   676.679016   904.086426   496.061737  3089.407471   \n",
       "2          138.0   594.518494   899.370361   453.098755  3599.037109   \n",
       "3          138.0   612.617310   969.024719   457.345673  4026.049316   \n",
       "4          138.0   591.839478   699.641968   391.703705  2411.037109   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "595        138.0  3547.037109  3536.123535  3399.456787  5190.913574   \n",
       "596        138.0  3677.234619  3715.728516  3639.283936  5169.777832   \n",
       "597        138.0  3955.530762  3922.024658  3810.567871  5253.185059   \n",
       "598        138.0  3752.493896  3682.567871  3530.469238  4916.493652   \n",
       "599        138.0  3206.246826  3110.123535  2891.407471  4446.296387   \n",
       "\n",
       "     WVP_w9_0115  visual_w9_0115  B05_w9_0115  B06_w9_0115  B07_w9_0115  ...  \\\n",
       "0    3326.357910       47.592594  1186.308594  3340.926025  4031.987549  ...   \n",
       "1    3345.357910       50.901234  1211.271606  2910.061768  3468.333252  ...   \n",
       "2    3260.691406       46.592594  1132.012329  2945.061768  3540.209961  ...   \n",
       "3    3344.580322       46.987656  1221.802490  3488.481445  4222.728516  ...   \n",
       "4    3597.086426       40.271606   754.666687  1883.036987  2293.296387  ...   \n",
       "..           ...             ...          ...          ...          ...  ...   \n",
       "595  3244.000000      255.000000  4135.827148  4902.740723  5308.320801  ...   \n",
       "596  3244.000000      255.000000  4065.308594  4804.962891  5194.530762  ...   \n",
       "597  3244.000000      255.000000  4083.481445  4751.197754  5104.567871  ...   \n",
       "598  3244.000000      255.000000  4017.938232  4671.617188  5010.147949  ...   \n",
       "599  3244.000000      239.876541  3738.222168  4432.777832  4786.654297  ...   \n",
       "\n",
       "     vh_w9_0520  vv_w9_0520  vh_w9_0601  vv_w9_0601  vh_w9_0818  vv_w9_0818  \\\n",
       "0      0.036744    0.157805    0.031072    0.078085    0.007117    0.038904   \n",
       "1      0.028177    0.349164    0.027931    0.092954    0.008786    0.089624   \n",
       "2      0.036251    0.199289    0.031167    0.148355    0.006448    0.020774   \n",
       "3      0.029057    0.136654    0.024184    0.078517    0.010887    0.050735   \n",
       "4      0.032237    0.182050    0.029203    0.071724    0.011458    0.096539   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "595    0.077351    0.296637    0.089087    0.326931    0.072536    0.369975   \n",
       "596    0.072449    0.313707    0.087714    0.358288    0.075839    0.311691   \n",
       "597    0.071250    0.307097    0.079684    0.337885    0.077614    0.263658   \n",
       "598    0.086430    0.282350    0.074414    0.339348    0.073026    0.258110   \n",
       "599    0.074432    0.297338    0.074617    0.320459    0.077615    0.333372   \n",
       "\n",
       "     vh_w9_1205  vv_w9_1205  vh_w9_1229  vv_w9_1229  \n",
       "0      0.036330    0.177955    0.006644    0.037547  \n",
       "1      0.016413    0.055847    0.004707    0.055702  \n",
       "2      0.039174    0.153203    0.004183    0.022632  \n",
       "3      0.049119    0.186494    0.009598    0.067469  \n",
       "4      0.008988    0.039027    0.004649    0.032278  \n",
       "..          ...         ...         ...         ...  \n",
       "595    0.058265    0.278032    0.052680    0.255063  \n",
       "596    0.066887    0.242163    0.068978    0.226601  \n",
       "597    0.065535    0.248294    0.080965    0.230987  \n",
       "598    0.065576    0.230867    0.078133    0.258629  \n",
       "599    0.066109    0.284618    0.069600    0.308956  \n",
       "\n",
       "[600 rows x 198 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentinel-2 data (training)\n",
    "cloud15S2_paths, cloud15S2_list = read_multiple_pickles('../11-datasets/ready_dataset/ready_dataset_cloud15', \n",
    "                                                     ['latitude', 'longitude', 'geometry', 'grouping'])\n",
    "\n",
    "cloud15_s2_df, cloud15_s2_df_list = batch_aggregate_pickle(cloud15S2_list, cloud15S2_paths, '_w9', None, agg_method=lambda x:x.mean())\n",
    "cloud_15_s2_y = pd.read_pickle(cloud15S2_paths[0])['Class of Land'].map({'Rice': 1, 'Non Rice': 0}).values\n",
    "\n",
    "# Sentinel-1 data (training)\n",
    "cloud15S1_paths, cloud15S1_list = read_multiple_pickles('../11-datasets/ready_dataset/ready_dataset_cloud15S1', \n",
    "                                                     ['latitude', 'longitude', 'geometry', 'grouping'])\n",
    "\n",
    "cloud15_s1_df, cloud15_s1_df_list = batch_aggregate_pickle(cloud15S1_list, cloud15S1_paths, '_w9')\n",
    "cloud_15_s1_y = pd.read_pickle(cloud15S2_paths[0])['Class of Land'].map({'Rice': 1, 'Non Rice': 0}).values\n",
    "\n",
    "# concat data from sentinel-2 and sentinel-1\n",
    "cloud15_s1s2 = pd.concat([cloud15_s2_df, cloud15_s1_df.drop('Class of Land', axis=1)], axis=1)\n",
    "cloud15_s1s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 data (template)\n",
    "cloud15S2t_paths, cloud15S2t_list = read_multiple_pickles('../11-datasets/ready_dataset/ready_dataset_cloud15t', \n",
    "                                                     ['latitude', 'longitude', 'geometry', 'grouping', 'target'])\n",
    "cloud15_s2t_df, cloud15_s2t_df_list = batch_aggregate_pickle(cloud15S2t_list, cloud15S2t_paths, '_w9', None)\n",
    "\n",
    "\n",
    "# Sentinel-1 data (template)\n",
    "cloud15S1t_paths, cloud15S1t_list = read_multiple_pickles('../11-datasets/ready_dataset/ready_dataset_cloud15S1t', \n",
    "                                                     ['latitude', 'longitude', 'geometry', 'grouping', 'target'])\n",
    "cloud15_s1t_df, cloud15_s1t_df_list = batch_aggregate_pickle(cloud15S1t_list, cloud15S1_paths, '_w9', None)\n",
    "\n",
    "# concat data\n",
    "cloud15_s1s2t = pd.concat([cloud15_s2t_df, cloud15_s1t_df], axis=1)\n",
    "X_pred6 = cloud15_s1s2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model6 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs = -1)\n",
    "model6.fit(cloud15_s1s2, cloud_15_s1_y)\n",
    "\n",
    "pred6 = model6.predict(X_pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df6 = prediction_to_submission_df('../submission/challenge_1_submission_template.csv', pred6)\n",
    "# submission_df6.to_csv('submission_csv/L1_Submission_6.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RFCloudless_model.h5']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import joblib\n",
    "# joblib.dump(model6, 'RFCloudless_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iteration 4.1 (extra exploration to see if we can get the same accuracy)\n",
    "\n",
    "The predicted value is the same as the one that has 1.0 f1-score. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add NDVI + VH/VV before feaeture selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of high_corr_cols: 83\n"
     ]
    }
   ],
   "source": [
    "X_train6_2 = pd.concat([add_NDVI(cloud15_s2_df_list), add_vhvv(cloud15_s1_df_list)], axis=1)\n",
    "X_pred6_2 = pd.concat([add_NDVI(cloud15_s2t_df_list), add_vhvv(cloud15_s1t_df_list)], axis=1)\n",
    "\n",
    "\n",
    "high_corr6_2 = get_high_corr_cols(X_train6_2, 0.95)\n",
    "\n",
    "X_train6_2 = X_train6_2.drop(high_corr6_2, axis=1)\n",
    "X_pred6_2 = X_pred6_2.drop(high_corr6_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model6.fit(X_train6_2, cloud_15_s1_y)\n",
    "pred_6_2 = model6.predict(X_pred6_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
