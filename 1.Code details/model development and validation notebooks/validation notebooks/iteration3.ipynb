{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7mUFCXPmgccF"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n","from sklearn.metrics import (accuracy_score, classification_report,\n","                             confusion_matrix, f1_score)\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","import lightgbm as lgb\n","import numpy as np\n","import pandas as pd\n","\n","\n","# download custom functions\n","import sys\n","sys.path.append('../model_dev_functions')\n","\n","from model_development import base_model_pred, df_to_arr, model_scores\n","from feature_engineering import add_NDVI, add_vhvv, get_high_corr_cols\n","from data_prep import (batch_aggregate_pickle, get_aggregation_from_window,\n","                       read_multiple_pickles)\n","from submission_format import prediction_to_submission_df\n","from temp_acc import temp_accuracy   # evaluate accuracy based on the prediction we used to get 1.0 accuracy score"]},{"cell_type":"markdown","metadata":{"id":"gTWx6wKogccH"},"source":["## iteration 3\n","test score = 0.94 / 0.95\n","\n","we tried adding Sentinel-1 data to the model stacking method used in iteration 2, which improves our model performance significantly. F1-score = 0.94 /  0.95 based on the normalization method we use (MinMax and RobustScaler performs better). To help readability, we will only illustrate experimentation with MinMaxScaler. \n","\n","- Sentinel-2: Feb, Aug, Dec raw data + NDVI \n","- Sentinel-1: Sentinel-1 raw data data + VH / VV\n","- drop highly correlated features\n","- normalization (StandardScaler, MinMaxScaler, RobustScaler)\n","- stack models\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KjnvILeZgccL"},"source":["#### Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"X68c95NKgccL"},"source":["training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kexrYA5ngccM"},"outputs":[],"source":["# get data for february, august and december for Sentinel-2 (training data)\n","fad_s2_paths, fad_s2_dfs_list = read_multiple_pickles('../11-datasets/feb_aug_dec-S2', ['latitude', 'longitude', 'geometry', 'grouping'])     # read multiple pickle files for band data corresponding to available dates in february, august and december\n","fad_s2_df, fad_s2_df_list  = batch_aggregate_pickle(fad_s2_dfs_list, fad_s2_paths, '_w5', 'Class of Land', agg_method=lambda x:x.mean())      # aggregate all the features with windows 5*5 by mean for the list of dataframes read from the pickle files\n","\n","# get data for february, august and december for Sentinel-1 (training data)\n","fad_s1_paths, fad_s1_dfs_list = read_multiple_pickles('../11-datasets/sentinel1a_data', \n","                                                        ['latitude', 'longitude', 'geometry', 'grouping'], filter_condition='-03-')     # read multiple pickle files for band data corresponding to available dates in february, august and december\n","fad_s1_df, fad_s1_df_list  = batch_aggregate_pickle(fad_s1_dfs_list, fad_s1_paths, '_w5', 'Class of Land', agg_method=lambda x:x.mean())      # aggregate all the features with windows 5*5 by mean for the list of dataframes read from the pickle files\n"]},{"cell_type":"markdown","metadata":{"id":"UF9XqusPgccM"},"source":["template data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mV9eA_SgccN"},"outputs":[],"source":["# get data for february, march and december for Sentinel-2 (coordinates from submission template)\n","sub2_fmad_paths, sub2_fmad_df_list = read_multiple_pickles('../11-datasets/SUBMISSION-template_data', \n","                                                         ['id', 'latitude', 'longitude', 'geometry', 'target'], filter_condition='-03-')\n","template_data_s2_df, template_s2_df_list = batch_aggregate_pickle(sub2_fmad_df_list, sub2_fmad_paths, '_w5', None)\n","\n","# get data for february, march and december for Sentinel-1\n","sub1_fad_paths, sub1_fad_df_list = read_multiple_pickles('../11-datasets/SUBMISSION-template_data/sentinel1a_template_data', \n","                                                         ['id', 'latitude', 'longitude', 'geometry', 'target'], filter_condition='-03-')\n","template_data_s1_df, template_s1_df_list = batch_aggregate_pickle(sub1_fad_df_list, sub1_fad_paths, '_w5', None)"]},{"cell_type":"markdown","metadata":{"id":"U288uRSAgccN"},"source":["#### model development"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CdJ34zbgccN","outputId":"64f05bcb-711a-4766-c871-91c46d86a73e"},"outputs":[{"name":"stdout","output_type":"stream","text":["number of high_corr_cols: 70\n"]}],"source":["def train_3_processing(list_s2_to_add_NDVI, s1_X_in, y_in, corr_thresh):\n","    X3 = pd.concat([add_NDVI(list_s2_to_add_NDVI), s1_X_in], axis=1)\n","    high_corr_cols = get_high_corr_cols(X3, corr_thresh)\n","    X3 = X3.drop(high_corr_cols, axis=1)\n","\n","    y3 = y_in.copy()\n","\n","    return X3, y3, high_corr_cols\n","\n","X_train3_prescaled, y_train3, corr_cols3 =  train_3_processing(fad_s2_df_list, add_vhvv(fad_s1_df_list), fad_s1_df['Class of Land'], corr_thresh = 0.95)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZU1ALSfCgccO"},"outputs":[],"source":["def pred_3_processing(list_s2_to_add_NDVI, sub1_X_in, corr_cols):\n","    X3 = pd.concat([add_NDVI(list_s2_to_add_NDVI), sub1_X_in], axis=1)\n","    X3 = X3.drop(corr_cols, axis=1)\n","\n","    return X3\n","\n","X_pred3_prescaled = pred_3_processing(template_s2_df_list, add_vhvv(template_s1_df_list), corr_cols3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qVRUWVJgccP","outputId":"574e174f-38ff-4c1c-aa58-e5c41d3ef2e2"},"outputs":[{"data":{"text/plain":["111"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["len(X_pred3_prescaled.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUvelOE7gccP"},"outputs":[],"source":["scale = MinMaxScaler()\n","scale.fit(X_train3_prescaled)\n","\n","X_train3= scale.transform(X_train3_prescaled)\n","X_pred3 = scale.transform(X_pred3_prescaled)\n","y_train3 = y_train3"]},{"cell_type":"markdown","metadata":{"id":"_cmXDxFDgccQ"},"source":["#### model development and prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdA18S2WgccQ","outputId":"18588a47-45c4-4157-c85b-25ed98bbf87c"},"outputs":[{"data":{"text/plain":["(250, 3)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# train base models\n","models = [\n","    RandomForestClassifier(random_state = 42, n_estimators=100),\n","    LogisticRegression(random_state = 42 ),\n","    #xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs = -1),\n","    lgb.LGBMClassifier(random_state = 42, n_estimators=100)\n","]\n","\n","\n","def base_model_pred_for_submission(X_train, y_train, X_pred, models):\n","    # Generate predictions from base models\n","    X = []  # This will store the prediction outputs of each model\n","    y = []  # This will store the true labels\n","    for model in models:\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_pred)\n","        X.append(y_pred)\n","\n","    X = np.array(X).T  # Convert to a 2D array, where each row represents a sample and each column represents a model's prediction\n","    # y = y_train  # Flatten the true labels into a 1D array\n","\n","    return X #, y\n","\n","X_pred3_2 = base_model_pred_for_submission(X_train3, y_train3, X_pred3, models)\n","X_pred3_2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0Oh9rtQgccR"},"outputs":[],"source":["X_train3_2, y_train3_2 = base_model_pred(X_train3, y_train3, models)\n","\n","models[0].fit(X_train3_2, y_train3_2)\n","meta_rf = models[0].predict(X_pred3_2) \n","\n","# models[1].fit(X_train2_2, y_train2_2)\n","# meta_lr = models[1].predict(X_pred2_2)\n","\n","# pred2 = (meta_rf + meta_lr) / 2\n","pred3 = meta_rf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKgW8ECogccS","outputId":"cac7c7af-6eff-41b0-ae2f-2c3e8e28499c"},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy=0.964, F1-score=0.9528795811518325\n"]}],"source":["print(f'accuracy={temp_accuracy(pred3)[0]}, F1-score={temp_accuracy(pred3)[1]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0Ic6TOegccS","outputId":"dab24d7e-6670-4d3c-95cf-1ba7ce395f61"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(10.18019073690894, 105.32022315786804)</td>\n","      <td>Rice</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(10.561107033461816, 105.12772097986661)</td>\n","      <td>Rice</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(10.623790611954897, 105.13771401411867)</td>\n","      <td>Rice</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(10.583364246115156, 105.23946127195805)</td>\n","      <td>Non Rice</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(10.20744446668854, 105.26844107128906)</td>\n","      <td>Rice</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>245</th>\n","      <td>(10.308283266873062, 105.50872812216863)</td>\n","      <td>Non Rice</td>\n","    </tr>\n","    <tr>\n","      <th>246</th>\n","      <td>(10.582910017285496, 105.23991550078767)</td>\n","      <td>Non Rice</td>\n","    </tr>\n","    <tr>\n","      <th>247</th>\n","      <td>(10.581547330796518, 105.23991550078767)</td>\n","      <td>Non Rice</td>\n","    </tr>\n","    <tr>\n","      <th>248</th>\n","      <td>(10.629241357910818, 105.15315779432643)</td>\n","      <td>Rice</td>\n","    </tr>\n","    <tr>\n","      <th>249</th>\n","      <td>(10.574733898351617, 105.10410108072531)</td>\n","      <td>Non Rice</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>250 rows × 2 columns</p>\n","</div>"],"text/plain":["                                           id    target\n","0     (10.18019073690894, 105.32022315786804)      Rice\n","1    (10.561107033461816, 105.12772097986661)      Rice\n","2    (10.623790611954897, 105.13771401411867)      Rice\n","3    (10.583364246115156, 105.23946127195805)  Non Rice\n","4     (10.20744446668854, 105.26844107128906)      Rice\n","..                                        ...       ...\n","245  (10.308283266873062, 105.50872812216863)  Non Rice\n","246  (10.582910017285496, 105.23991550078767)  Non Rice\n","247  (10.581547330796518, 105.23991550078767)  Non Rice\n","248  (10.629241357910818, 105.15315779432643)      Rice\n","249  (10.574733898351617, 105.10410108072531)  Non Rice\n","\n","[250 rows x 2 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# load submission\n","submission_df3 = prediction_to_submission_df('../submission/challenge_1_submission_template.csv', pred3)\n","submission_df3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwZFr3LRgccT"},"outputs":[],"source":["# submission_df3.to_csv(\"L1_Submission_3.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"py310","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}